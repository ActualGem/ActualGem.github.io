---
title: 'Optimizing an STDP Learning Engine for FPGAs'
date: 2025-06-26
permalink: /posts/2025/06/optimizing-an-stdp-learning-engine-for-fpgas/
tags:
  - FPGAS
  - Spiking Neural Networks
  - STDP
---

Written by Aaryan Dhawan, in collaboration with Muhammad Farhan Azmine

What is STDP?
===============

Spike-Timing-Dependent Plasticity (STDP) is a biological learning rule that adjusts the strength of connections between neurons based on the relative timing of their spikes. In simple terms, if neuron A fires just before neuron B, the connection between them is strengthened. Conversely, if neuron A fires after neuron B, the connection is weakened. In machine learning, STDP is used to train Spiking Neural Networks (SNNs) by mimicking this biological process, allowing the network to learn from temporal patterns in data. 
Using temporal patterns to learn, STDP allows SNNs to process information similarly to the human brain, making them suitable for tasks like real-time learning, robotics, and in edge computing.

On-Chip STDP Learning 
----------

On-chip STDP learning refers to the implementation of the STDP learning algorithm directly within a hardware implementation of a SNN, rather than training the network in software and transferring it to hardware. Implementing the learning algorithm can reduce the need for large datasets, as the network can learn from the data it processes in real-time. This approach is particularly beneficial for applications with limited data availability or when power consumption is a concern. In 2019, researches at Texas A&M University developed FPGA efficient, STDP based SNN models for both supervised and unsupervised learning [1](https://dl.acm.org/doi/10.1145/3313866) with Liquid State machines (LSMs). In this work, they build a STDP learning engine that takes the spike timing difference between the reservoir neurons and the output neurons, and uses that to determine between strengthening (Potentiation) or weakening (Depression) the connection between the two neurons. This adjustment value is determine by the following equations:
$$\Delta w = \begin{cases}
\eta_{+} \cdot e^{-\frac{t_{pre}-t_{post}}{\tau_{+}}} & \text{if } t_{pre} < t_{post} \\
-\eta_{-} \cdot e^{-\frac{t_{post}-t_{pre}}{\tau_{-}}} & \text{if } t_{pre} > t_{post}
\end{cases}$$

where:
- $$\Delta w$$ is the change in weight
- $$t_{pre}$$ is the time of the pre-synaptic spike
- $$t_{post}$$ is the time of the post-synaptic spike
- $$\tau_{+}$$ is the time constant for potentiation
- $$\tau_{-}$$ is the time constant for depression
- $$\eta_{+}$$ is the amplitude of the potentiation curve
- $$\eta_{-}$$ is the amplitude of the depression curve

Over a given sample period, the neurons will produce spike trains, which are compared to each other to compute a timing difference value, $$\Delta t$$. This $$\Delta t$$ is used to calculate the potentiation or depression value, strengthening or weakening a given connection. Determining potentiation vs depression can be visualized as follows:
![STDP Simulation](/images/STDP_learning.gif)
Spiking before the post-synaptic neuron results in a positive $$\Delta t$$, leading to potentiation, while spiking after results in a negative $$\Delta t$$, leading to depression.

Original Hardware Design
---------

In [1](https://dl.acm.org/doi/10.1145/3313866), the authors develop a "hardware-friendly STDP learning engine", recreated in the following image.
![STDP Learning Engine](/images/SotA_STDP_LE_diagram.png)

Discrete spike trains from the reservoir neurons and output neurons are fed into Serial-In/Parallel-Out (SIPO) shift registers, the "length" of the registers are equal to the number of timesteps in a given sample period (For HW, a fixed sample period). The shift registers corresponding to the reservior neurons are then selected by an M to 1 multiplexer, where M is the number of reservoir neurons. Once the shift register is selected and its data is passed through the multiplexer, the two spike trains, 1 post-synaptic and 1 pre-synaptic, are compared to each other to determine the $$\Delta t$$ to adjust that given weight. This is done by comparator, which looks as the Most Significant bit(MSB) of the two spike trains and looks at which of the two is high/Logic-1. If the Pre-synaptic neuron MSB is high, then the $$\Delta t$$ will be negative and the post-synaptic spike train will be sent to negative priority encoder; If the Post-synaptic neuron MSB is high, then the $$\Delta t$$ will be positive and the pre-synaptic spike train will be sent to positive priority encoder.  

Rather than implementing hardware to compute the potention/depression values with the 2 equations above, the authors use two Look-Up Tables (LUTs), one for potentiation and one for depression, to easily compute $$\Delta w$$ as there is only a certain range of possible values. $$\Delta w$$ is then added to synaptic connection, read from a memory block, using a simple adder. The synaptic connection is then written back to the memory block, completing the learning process for that given connection.


Optimizing the Learning Engine
----------

Looking at this design, we can identify a few areas for optimization, specifically in the timing difference calculation and the potentiation/depression value calculation. Seeing that the Depression LUT has the same values as the Potentiation LUT but negative, we can remove this second negative LUT. Thus, the comparator utilization can be reduced as we replace the negative priority encoder with a “signed flag” signal, sent to the simple adder before the weight memory. 
Optimizations:
-Reduce the Potentiation/Depression LUT size
-Replace a priority encoder with a single-bit, flag signal.
This results in the following design:
![Optimized STDP Learning Engine](/images/Optimized_STDP_LE_diagram.png)
In addtion, we can add a pipleline register before the adder to reduce the critical path delay and increase the maximum clock frequency of the design.
![Optimized STDP Learning Engine with Pipeline](/images/Optimized_STDP_LE_PL_diagram.png)

FPGA Utilization
----------
We designed, synthesized, and implemented the original and optimized STDP learning engines on a Xilinx Zynq-7000 SoC "ZC702" Evaulation board. We also implemented a pipleining register in the orginial design to get better comparisons between the designs. The following table shows the FPGA resource utilization for both designs:

| Resource Type | Original Design  | Original Design With Pipeline | Optimized Design with Pipeline |
|---------------|------------------|-------------------------------|--------------------------------|
| LUTs          | 182              | 183                           | 169                            |
| FFs           | 318              | 351                           | 351                            |
| Fmax          | 186 MHz          | 180 MHz                       | 222 MHz                        |

